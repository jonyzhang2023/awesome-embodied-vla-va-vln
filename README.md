# awesome-embodied-vla/va/vln [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

As more and more outstanding vision-language-based policies emerge, this repository aims to organize and showcase the state-of-the-art technologies in robot learning, including vision-language-action (VLA) models, vision-language-navigation (VLN) models, vision-action (VA) models and other MLLM-based embodied learning. We hope that in the near future, robotics will experience its own 'LLM moment.'

This repository will be continuously updated, and we warmly invite contributions from the community. If you have any papers, projects, or resources that are not yet included, please feel free to submit them via a pull request or open an issue for discussion. 

Let's build a comprehensive resource for the robotics and AI community!

*Jony and Sage*


## Survey

- [2025] Multimodal Perception for Goal-oriented Navigation: A Survey [[paper](https://arxiv.org/pdf/2504.15643)]
- [2025] Diffusion Models for Robotic Manipulation: A Survey [[paper](https://arxiv.org/pdf/2504.08438)]
- [2025] Dexterous Manipulation through Imitation Learning: A Survey [[paper](https://arxiv.org/pdf/2504.03515)]
- [2025] Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision [[paper](https://arxiv.org/pdf/2504.02477)] [[project](https://github.com/Xiaofeng-Han-Res/MF-RV)] 
- [2025] SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey [[paper](https://arxiv.org/pdf/2503.09829)]
- [2025] Generative Artificial Intelligence in Robotic Manipulation: A Survey [[paper](https://arxiv.org/pdf/2503.03464)] [[project](https://github.com/GAI4Manipulation/AwesomeGAIManipulation)] 
- [2025] Development Report of Embodied Intelligence (Chinese) [[paper](https://www.caict.ac.cn/kxyj/qwfb/bps/202408/P020240830312499650772.pdf)]
- [2025] Survey on Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2502.06851)]
- [2025] Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions [[paper](https://arxiv.org/pdf/2502.15336)]
- [2024] Embodied-AI with large models: research and challenges [[paper](https://www.sciengine.com/SSI/doi/10.1360/SSI-2024-0076)]
- [2024] A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- [2024] A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- [2024] Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- [2024] Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

## Vision Language Action (VLA) Models

### 2025 

- [2025] [**Physical Intelligence**] π0.5: A Vision-Language-Action Model with Open-World Generalization [[paper](https://arxiv.org/pdf/2504.16054)] [[project](https://www.pi.website/blog/pi05)] 
- [2025] [**Nvidia**] GR00T N1: An Open Foundation Model for Generalist Humanoid Robots [[paper](https://arxiv.org/pdf/2503.14734)] [[project](https://github.com/NVIDIA/Isaac-GR00T)] 
- [2025] [**Gemini Robotics**] Gemini Robotics: Bringing AI into the Physical World [[report](https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf)]
- [2025] [**AgiBot**] AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems [[paper](https://agibot-world.com/blog/agibot_go1.pdf)] [[project](https://agibot-world.com/blog/go1)] 
- [2025] [**PsiBot**] DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping [[paper](https://arxiv.org/pdf/2502.20900)] [[project](https://dexgraspvla.github.io/)] 
- [2025] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success [[paper](https://arxiv.org/pdf/2502.19645)] [[project](https://openvla-oft.github.io/)] 
- [2025] [**Physical Intelligence**] Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2502.19417)] [[project](https://www.pi.website/research/hirobot)] 
- [2025] [**Figure**] Helix: A Vision-Language-Action Model for Generalist Humanoid Control [[report](https://www.figure.ai/news/helix)]
- [2025] [**AgiBot**] EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation [[paper](https://arxiv.org/pdf/2501.01895)]
- [2025] Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing [[paper](https://arxiv.org/pdf/2501.06919)]
- [2025] Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding [[paper](https://arxiv.org/pdf/2501.04693)]
- [2025] [**Physical Intelligence**] FAST: Efficient Action Tokenization for Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2501.09747)]
- [2025] GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation [[paper](https://arxiv.org/pdf/2501.09783)]
- [2025] Universal Actions for Enhanced Embodied Foundation Models [[paper](https://arxiv.org/pdf/2501.10105)]
- [2025] SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model [[paper](https://arxiv.org/pdf/2501.15830)]
- [2025] RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation [[paper](https://arxiv.org/pdf/2501.06605)]
- [2025] SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation [[paper](https://arxiv.org/pdf/2501.18564)]
- [2025] Improving Vision-Language-Action Model with Online Reinforcement Learning [[paper](https://arxiv.org/pdf/2501.16664)]
- [2025] Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation [[paper](https://arxiv.org/pdf/2501.18733)]
- [2025] VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation [[paper](https://arxiv.org/pdf/2502.02175)]
- [2025] From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment [[paper](https://arxiv.org/pdf/2502.01828)]
- [2025] GRAPE: Generalizing Robot Policy via Preference Alignment [[paper](https://arxiv.org/pdf/2411.19309)]
- [2025] DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control [[paper](https://arxiv.org/pdf/2502.05855)]
- [2025] HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation [[paper](https://arxiv.org/pdf/2502.05485)]
- [2025] Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following Temporal Representation Alignment [[paper](https://arxiv.org/pdf/2502.05454)]
- [2025] ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[paper](https://arxiv.org/pdf/2502.05450)]
- [2025] RoboBERT: An End-to-end Multimodal Robotic Manipulation Model [[paper](https://arxiv.org/pdf/2502.07837)]
- [2025] Diffusion Transformer Policy: Scaling Diffusion Transformer for Generalist Visual-Language-Action Learning [[paper](https://arxiv.org/pdf/2410.15959)]
- [2025] GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation [[paper](https://arxiv.org/pdf/2502.09268)]
- [2025] SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation [[paper](https://arxiv.org/pdf/2502.09268)]
- [2025] Pre-training Auto-regressive Robotic Models with 4D Representations [[paper](https://arxiv.org/pdf/2502.13142)]
- [2025] Magma: A Foundation Model for Multimodal AI Agents [[paper](https://arxiv.org/pdf/2502.13130)]
- [2025] An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation [[paper](https://arxiv.org/pdf/2501.15068)]
- [2025] VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation [[paper](https://arxiv.org/pdf/2502.13508v1)]
- [2025] Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration [[paper](https://arxiv.org/pdf/2502.14795)]
- [2025] ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2502.14420)]
- [2025] ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration [[paper](https://arxiv.org/pdf/2502.19250)] [[project](https://objectvla.github.io/)] 
- [2025] RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete [[paper](https://arxiv.org/pdf/2502.21257)] [[project](https://superrobobrain.github.io/)] 
- [2025] SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning [[paper](https://arxiv.org/pdf/2503.03480)] [[project](https://sites.google.com/view/pku-safevla)] 
- [2025] CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs [[paper](https://arxiv.org/pdf/2503.01378)] [[project](https://cognitivedrone.github.io/)] 
- [2025] UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation [[paper](https://arxiv.org/pdf/2501.05014)]
- [2025] Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding [[paper](https://arxiv.org/pdf/2503.02310)]
- [2025] RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour [[paper](https://arxiv.org/pdf/2503.02572)] [[project](https://racevla.github.io/)] 
- [2025] OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction [[paper](https://arxiv.org/pdf/2503.03734)] [[project](https://ottervla.github.io/)] 
- [2025] A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery [[paper](https://arxiv.org/pdf/2503.03579)] [[project](https://i3handover.github.io/)] 
- [2025] VLA Model-Expert Collaboration for Bi-directional Manipulation Learning [[paper](https://arxiv.org/pdf/2503.04163)] [[project](https://aoqunjin.github.io/Expert-VLA/)]
- [2025] PointVLA: Injecting the 3D World into Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2503.07511)] [[project](https://pointvla.github.io/)]
- [2025] Towards Safe Robot Foundation Models [[paper](https://arxiv.org/pdf/2503.07404)] [[project](https://sites.google.com/robot-learning.de/towards-safe-rfm)]
- [2025] VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.07135)] [[project](https://hanzhic.github.io/vidbot-project/)]
- [2025] iManip: Skill-Incremental Learning for Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.07087)]
- [2025] Refined Policy Distillation: From VLA Generalists to RL Experts [[paper](https://arxiv.org/pdf/2503.05833)]
- [2025] LUMOS: Language-Conditioned Imitation Learning with World Models [[paper](https://arxiv.org/pdf/2503.10370)] [[project](http://lumos.cs.uni-freiburg.de/)]
- [2025] HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2503.10631)] [[project](https://hybrid-vla.github.io/)]
- [2025] MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2503.08007)]
- [2025] TLA: Tactile-Language-Action Model for Contact-Rich Manipulation [[paper](https://arxiv.org/pdf/2503.08548)] [[project](https://sites.google.com/view/tactile-language-action/)]
- [2025] FP3: A 3D Foundation Policy for Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.08950)] [[project](https://3d-foundation-policy.github.io/)]
- [2025] MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation [[paper](https://arxiv.org/pdf/2503.13446)] [[project](https://gary3410.github.io/momanipVLA/)]
- [2025] Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy [[paper](https://arxiv.org/pdf/2410.15959v4)] [[project](https://robodita.github.io/)]
- [2025] Think Small, Act Big: Primitive-level Skill Prompt Learning for Lifelong Robot Manipulation [[paper](https://openreview.net/pdf?id=tpUEqmjZiS)]
- [2025] JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse [[paper](https://arxiv.org/pdf/2503.16365)] [[project](https://craftjarvis.github.io/JarvisVLA/)]
- [2025] RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.19510)]
- [2025] CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model [[paper](https://arxiv.org/pdf/2503.19225)]
- [2025] MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation [[paper](https://arxiv.org/pdf/2503.20384)] [[project](https://sites.google.com/view/mole-vla)]
- [2025] ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[paper](https://arxiv.org/abs/2502.05450)] [[project](https://cccedric.github.io/conrft/)]
- [2025] CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2503.22020)] [[project](https://cot-vla.github.io/)]
- [2025] DexTOG: Learning Task-Oriented Dexterous Grasp with Language Condition [[paper](https://arxiv.org/pdf/2504.04573)] [[project](https://sites.google.com/view/dextog)]
- [2025] SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation [[paper](https://arxiv.org/pdf/2504.15561)]
- [2025] Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation [[paper](https://arxiv.org/pdf/2504.17784)] [[project](https://yuyinyang3y.github.io/PPI/)]

### 2024

- [2024] [**Physical Intelligence**] π0: A Vision-Language-Action Flow Model for General Robot Control [[paper](https://www.physicalintelligence.company/download/pi0.pdf)]
- [2024] RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation [[paper](https://arxiv.org/pdf/2410.07864)]
- [2024] OpenVLA: An Open-Source Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2406.09246)]
- [2024] Octo: An Open-Source Generalist Robot Policy [[paper](https://arxiv.org/pdf/2405.12213)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] RT-H: Action Hierarchies Using Language [[paper](https://arxiv.org/pdf/2403.01823v1)]
- [2024] Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models [[paper](https://arxiv.org/abs/2412.14058)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] Baku: An Efficient Transformer for Multi-Task Policy Learning [[paper](https://arxiv.org/pdf/2406.07539v1)]
- [2024] Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals [[paper](https://arxiv.org/pdf/2407.05996)]
- [2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.12514)]
- [2024] Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression [[paper](https://arxiv.org/pdf/2412.03293)]
- [2024] CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation [[paper](https://www.arxiv.org/pdf/2411.19650)]
- [2024] 3D-VLA: A 3D Vision-Language-Action Generative World Model [[paper](https://arxiv.org/pdf/2403.09631)]
- [2024] Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations [[paper](https://arxiv.org/pdf/2405.06039)]
- [2024] An Embodied Generalist Agent in 3D World [[paper](https://arxiv.org/pdf/2311.12871)]
- [2024] RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2412.07215)]
- [2024] SpatialBot: Precise Spatial Understanding with Vision Language Models [[paper](https://arxiv.org/pdf/2406.13642)]
- [2024] Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection [[paper](https://arxiv.org/pdf/2408.05107v1)]
- [2024] HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers [[paper](https://arxiv.org/pdf/2410.05273)]
- [2024] LLaRA: Supercharging Robot Learning Data for Vision-Language Policy [[paper](https://arxiv.org/pdf/2406.20095)]
- [2024] RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.18977)]
- [2024] Robotic Control via Embodied Chain-of-Thought Reasoning [[paper](https://arxiv.org/pdf/2407.08693)]
- [2024] GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation [[paper](https://arxiv.org/pdf/2410.06158)]
- [2024] Latent Action Pretraining from Videos [[paper](https://arxiv.org/pdf/2410.11758)]
- [2024] DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution [[paper](https://arxiv.org/pdf/2411.02359)]
- [2024] RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation [[paper](https://arxiv.org/pdf/2411.02704)]
- [2024] Moto: Latent Motion Token as the Bridging Language for Robot Manipulation [[paper](https://arxiv.org/pdf/2412.04445)]
- [2024] TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies [[paper](https://arxiv.org/pdf/2412.10345)]
- [2024] Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments [[paper](https://arxiv.org/pdf/2409.05865)]
- [2024] Moto: Latent Motion Token as the Bridging Language for Robot Manipulation [[paper](https://arxiv.org/pdf/2412.04445)]
- [2024] RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics [[paper](https://arxiv.org/pdf/2406.10721)]
- [2024] Yell At Your Robot: Improving On-the-Fly from Language Corrections [[paper](https://arxiv.org/pdf/2403.12910)]
- [2024] Any-point Trajectory Modeling for Policy Learning [[paper](https://arxiv.org/pdf/2401.00025)]
- [2024] Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust [[paper](https://arxiv.org/pdf/2410.01971)]
- [2024] RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2409.19590)]
- [2024] Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning [[paper](https://arxiv.org/pdf/2408.01147)]
- [2024] QUAR-VLA: Vision-Language-Action Model for Quadruped Robots [[paper](https://arxiv.org/pdf/2312.14457)]
- [2024] RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2409.19590)]
- [2024] DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution [[paper](https://arxiv.org/pdf/2411.02359)]
- [2024] General Flow as Foundation Affordance for Scalable Robot Learning [[paper](https://arxiv.org/pdf/2401.11439)] [[project](https://general-flow.github.io/)]


### 2023 

- [2023] RT-1: Robotics Transformer for Real-World Control at Scale [[paper](https://arxiv.org/pdf/2212.06817)]
- [2023] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper](https://arxiv.org/pdf/2307.15818)]
- [2023] PaLM-E: An Embodied Multimodal Language Model: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper](https://arxiv.org/pdf/2303.03378)]
- [2023] Vision-Language Foundation Models as Effective Robot Imitators [[paper](https://arxiv.org/pdf/2311.01378)]
- [2023] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation [[paper](https://arxiv.org/pdf/2312.13139)]
- [2023] Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models [[paper](https://arxiv.org/pdf/2310.10639)]
- [2023] Learning Universal Policies via Text-Guided Video Generation [[paper](https://arxiv.org/pdf/2302.00111)]
- [2023] Learning to Act from Actionless Videos through Dense Correspondences [[paper](https://arxiv.org/pdf/2310.08576)]
- [2023] Compositional Foundation Models for Hierarchical Planning [[paper](https://arxiv.org/pdf/2309.08587)]
- [2023] VIMA: General Robot Manipulation with Multimodal Prompts [[paper](https://vimalabs.github.io./assets/vima_paper.pdf)]
- [2023] Prompt a Robot to Walk with Large Language Models [[paper](https://arxiv.org/pdf/2309.09969)]
- [2023] Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning [[paper](https://arxiv.org/pdf/2311.17842)]


## Vision Language Navigation (VLN) Models

### 2025

- [2025] Semantic Mapping in Indoor Embodied AI - A Comprehensive Survey and Future Directions [[paper](https://arxiv.org/pdf/2501.05750)]
- [2025] VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning [[paper](https://arxiv.org/pdf/2502.00931)]
- [2025] TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2502.07306)]
- [2025] VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion [[paper](https://arxiv.org/pdf/2502.01536)]
- [2025] NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants [[paper](https://arxiv.org/pdf/2502.13894)]
- [2025] MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2502.13451)]
- [2025] OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation [[paper](https://arxiv.org/pdf/2502.18041)]
- [2025] Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments [[paper](https://arxiv.org/pdf/2502.19024)]
- [2025] WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation [[paper](https://arxiv.org/pdf/2503.02247)] [[project](https://b0b8k1ng.github.io/WMNav/)]
- [2025] Dynamic Path Navigation for Motion Agents with LLM Reasoning [[paper](https://arxiv.org/pdf/2503.07323)]
- [2025] SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2503.10069)]
- [2025] Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments [[paper](https://arxiv.org/pdf/2503.09820)]
- [2025] UniGoal: Towards Universal Zero-shot Goal-oriented Navigation [[paper](https://arxiv.org/pdf/2503.10630)] [[project](https://bagh2178.github.io/UniGoal/)]
- [2025] PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation for Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2503.09938)]
- [2025] Do Visual Imaginations Improve Vision-and-Language Navigation Agents? [[paper](https://arxiv.org/pdf/2503.16394)] [[project](https://www.akhilperincherry.com/VLN-Imagine-website/)]
- [2025] HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard [[paper](https://arxiv.org/pdf/2503.14229)] [[project](https://ha-vln-project.vercel.app/)]
- [2025] FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks [[paper](https://arxiv.org/pdf/2503.13966)]
- [2025] P3Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction [[paper](https://arxiv.org/pdf/2503.18525)]
- [2025] Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation [[paper](https://arxiv.org/pdf/2503.18065)] [[project](https://github.com/SaDil13/VLN-RAM)]
- [2025] COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2503.24065)]
- [2025] ForesightNav: Learning Scene Imagination for Efficient Exploration [[paper](https://arxiv.org/pdf/2504.16062)] [[project](https://github.com/uzh-rpg/foresight-nav)]

### 2024

- [2024] Navid: Video-based vlm plans the next step for vision-andlanguage navigation [[paper](https://arxiv.org/pdf/2402.15852)]
- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation [[paper](https://arxiv.org/pdf/2412.04453)]
- [2024] The One RING: a Robotic Indoor Navigation Generalist [[paper](https://arxiv.org/pdf/2412.14401)]
- [2024] Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs [[paper](https://arxiv.org/pdf/2407.07775)]

### 2023

- [2023] Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill [[paper](https://arxiv.org/pdf/2309.10309)]


## Vision Action (VA) Models

### 2025

- [2025] Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition [[paper](https://arxiv.org/pdf/2503.12466)] [[project](https://github.com/AndyCao1125/MCDP)] 
- [2025] Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning [[paper](https://arxiv.org/pdf/2503.04877)] [[project](https://www.pair.toronto.edu/Adapt3R/)] 
- [2025] BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities [[paper](https://arxiv.org/pdf/2503.05652)] [[project](https://behavior-robot-suite.github.io/)] 
- [2025] Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation [[paper](https://arxiv.org/pdf/2503.02881)] [[project](https://reactive-diffusion-policy.github.io/)] 
- [2025] Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics [[paper](https://arxiv.org/pdf/2501.10100)]
- [2025] You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations [[paper](https://arxiv.org/pdf/2501.14208)]
- [2025] ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills [[paper](https://arxiv.org/pdf/2502.01143)]
- [2025] VILP: Imitation Learning with Latent Video Planning [[paper](https://arxiv.org/pdf/2502.01784)]
- [2025] Learning the RoPEs: Better 2D and 3D Position Encodings with STRING [[paper](https://arxiv.org/pdf/2502.02562)]
- [2025] When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning [[paper](https://arxiv.org/pdf/2502.03270)]
- [2025] RoboGrasp: A Universal Grasping Policy for Robust Robotic Control [[paper](https://arxiv.org/pdf/2502.03072)]
- [2025] CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World [[paper](https://arxiv.org/pdf/2502.08449)]
- [2025] Learning to Group and Grasp Multiple Objects [[paper](https://arxiv.org/pdf/2502.08452)]
- [2025] Beyond Behavior Cloning: Robustness through Interactive Imitation and Contrastive Learning [[paper](https://arxiv.org/pdf/2502.07645)]
- [2025] COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping [[paper](https://arxiv.org/pdf/2502.08054)]
- [2025] DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References [[paper](https://arxiv.org/pdf/2502.09614)]
- [2025] S2-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation [[paper](https://arxiv.org/pdf/2502.09389)]
- [2025] MTDP: Modulated Transformer Diffusion Policy Model [[paper](https://arxiv.org/pdf/2502.09029)]
- [2025] FUNCTO: Function-Centric One-Shot Imitation Learning for Tool Manipulation [[paper](https://arxiv.org/pdf/2502.11744)]
- [2025] RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations [[paper](https://arxiv.org/pdf/2502.13134)]
- [2025] Responsive Noise-Relaying Diffusion Policy: Responsive and Efficient Visuomotor Control [[paper](https://arxiv.org/pdf/2502.12724)]
- [2025] Learning a High-quality Robotic Wiping Policy Using Systematic Reward Analysis and Visual-Language Model Based Curriculum [[paper](https://arxiv.org/pdf/2502.12599)]
- [2025] IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation [[paper](https://arxiv.org/pdf/2502.12371)]
- [2025] X-IL: Exploring the Design Space of Imitation Learning Policies [[paper](https://arxiv.org/pdf/2502.12330)]
- [2025] Towards Fusing Point Cloud and Visual Representations for Imitation Learning [[paper](https://arxiv.org/pdf/2502.12320)]
- [2025] Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach [[paper](https://arxiv.org/pdf/2502.15613)]
- [2025] FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning [[paper](https://arxiv.org/pdf/2502.17432)]
- [2025] DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning [[paper](https://arxiv.org/pdf/2502.16932)]
- [2025] Human2Robot: Learning Robot Actions from Paired Human-Robot Videos [[paper](https://arxiv.org/pdf/2502.16587)]
- [2025] AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-level Learning Efficiency [[paper](https://arxiv.org/pdf/2502.16420)]
- [2025] COMPASS: Cross-embOdiment Mobility Policy via ResiduAl RL and Skill Synthesis [[paper](https://arxiv.org/pdf/2502.16372)]
- [2025] Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand [[paper](https://arxiv.org/pdf/2502.18423)]
- [2025] From planning to policy: distilling Skill-RRT for long-horizon prehensile and non-prehensile manipulation [[paper](https://arxiv.org/pdf/2502.18015)]
- [2025] FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real [[paper](https://arxiv.org/pdf/2502.17894)]
- [2025] Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation [[paper](https://arxiv.org/pdf/2502.20391)] [[project](https://www.pi.website/research/hirobot)] 
- [2025] FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects [[paper](https://arxiv.org/pdf/2502.20037)]
- [2025] Sensor-Invariant Tactile Representation [[paper](https://arxiv.org/pdf/2502.19638)]
- [2025] Generalist World Model Pre-Training for Efficient Reinforcement Learning [[paper](https://arxiv.org/pdf/2502.19544)]
- [2025] ProDapt: Proprioceptive Adaptation using Long-term Memory Diffusion [[paper](https://arxiv.org/pdf/2503.00193)] [[project](https://github.com/Federico-PizarroBejarano/prodapt)] 
- [2025] Falcon: Fast Visuomotor Policies via Partial Denoising [[paper](https://arxiv.org/pdf/2503.00339)]
- [2025] HGDiffuser: Efficient Task-Oriented Grasp Generation via Human-Guided Grasp Diffusion Models [[paper](https://arxiv.org/pdf/2503.00508)] [[project](https://sites.google.com/view/hgdiffuser)] 
- [2025] SHADOW: Leveraging Segmentation Masks for Cross-Embodiment Policy Transfer [[paper](https://arxiv.org/pdf/2503.00774)] [[project](https://shadow-cross-embodiment.github.io/)] 
- [2025] Phantom: Training Robots Without Robots Using Only Human Videos [[paper](https://arxiv.org/pdf/2503.00779)] [[project](https://phantom-human-videos.github.io/)] 
- [2025] General Force Sensation for Tactile Robot [[paper](https://arxiv.org/pdf/2503.01058)]
- [2025] Action Tokenizer Matters in In-Context Imitation Learning [[paper](https://arxiv.org/pdf/2503.01206)]
- [2025] AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint and Focal Length Optimization [[paper](https://arxiv.org/pdf/2503.01439)] [[project](https://avr-robot.github.io/)]
- [2025] FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement Primitives for Smooth Action Generation [[paper](https://arxiv.org/pdf/2503.02048)]
- [2025] Variable-Friction In-Hand Manipulation for Arbitrary Objects via Diffusion-Based Imitation Learning [[paper](https://arxiv.org/pdf/2503.02738)] [[project](https://sites.google.com/view/vf-ihm-il/home)]
- [2025] Learning Dexterous In-Hand Manipulation with Multifingered Hands via Visuomotor Diffusion [[paper](https://arxiv.org/pdf/2503.02587)] [[project](https://dex-manip.github.io/)]
- [2025] RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB Image for Graspability-Aware Bin Picking [[paper](https://arxiv.org/pdf/2503.02387)] [[project](https://ilikesukiyaki.github.io/RGBSQGrasp/)]
- [2025] ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation [[paper](https://arxiv.org/pdf/2503.03045)] [[project](https://articubot.github.io/)] 
- [2025] SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks [[paper](https://arxiv.org/pdf/2503.04538)] [[project](https://srsa2024.github.io/)] 
- [2025] GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping [[paper](https://arxiv.org/pdf/2503.04123)] [[project](https://gagrasp.github.io/)] 
- [2025] OPG-Policy: Occluded Push-Grasp Policy Learning with Amodal Segmentation [[paper](https://arxiv.org/pdf/2503.04089)]
- [2025] RA-DP: Rapid Adaptive Diffusion Policy for Training-Free High-frequency Robotics Replanning [[paper](https://arxiv.org/pdf/2503.04051)]
- [2025] Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision and Force Observations [[paper](https://arxiv.org/pdf/2503.03998)] [[project](https://rros-lab.github.io/diffusion-with-force.github.io/)] 
- [2025] CoinRobot: Generalized End-to-end Robotic Learning for Physical Intelligence [[paper](https://arxiv.org/pdf/2503.05316)]
- [2025] Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects [[paper](https://arxiv.org/pdf/2503.05189)] [[project](https://berkeleyautomation.github.io/POGS/)] 
- [2025] How to Train Your Robots? The Impact of Demonstration Modality on Imitation Learning [[paper](https://arxiv.org/pdf/2503.07017)] 
- [2025] One-Shot Dual-Arm Imitation Learning [[paper](https://arxiv.org/pdf/2503.06831)] [[project](https://www.robot-learning.uk/one-shot-dual-arm)] 
- [2025] GAT-Grasp: Gesture-Driven Affordance Transfer for Task-Aware Robotic Grasping [[paper](https://arxiv.org/pdf/2503.06227)]
- [2025] Enhanced View Planning for Robotic Harvesting: Tackling Occlusions with Imitation Learning [[paper](https://arxiv.org/pdf/2503.10334)]
- [2025] ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and Spiking Neural Network [[paper](https://arxiv.org/pdf/2503.09985)]
- [2025] NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models [[paper](https://arxiv.org/pdf/2503.10626)]
- [2025] World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning [[paper](https://arxiv.org/pdf/2503.10480)]
- [2025] RILe: Reinforced Imitation Learning [[paper](https://arxiv.org/pdf/2406.08472)]
- [2025] HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots [[paper](https://arxiv.org/abs/2503.09010)]
- [2025] Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for Humanoid Robot Perceptive Locomotion [[paper](https://arxiv.org/abs/2503.08299)]
- [2025] Trinity: A Modular Humanoid Robot AI System [[paper](https://arxiv.org/abs/2503.08338)]
- [2025] LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures [[paper](https://arxiv.org/abs/2503.08349)]
- [2025] Elastic Motion Policy: An Adaptive Dynamical System for Robust and Efficient One-Shot Imitation Learning [[paper](https://arxiv.org/pdf/2503.08029)] [[project](https://elastic-motion-policy.github.io/EMP/)] 
- [2025] Learning Gentle Grasping Using Vision, Sound, and Touch [[paper](https://arxiv.org/pdf/2503.07926)] [[project](https://lasr.org/research/gentle-grasping)] 
- [2025] RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation [[paper](https://arxiv.org/pdf/2503.07771)]
- [2025] Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework [[paper](https://arxiv.org/pdf/2503.09186)]
- [2025] MoE-Loco: Mixture of Experts for Multitask Locomotion [[paper](https://arxiv.org/pdf/2503.08564)] [[project](https://moe-loco.github.io/)] 
- [2025] Humanoid Policy ~ Human Policy [[paper](https://arxiv.org/pdf/2503.13441)] [[project](https://human-as-robot.github.io/)] 
- [2025] Dense Policy: Bidirectional Autoregressive Learning of Actions [[paper](https://arxiv.org/pdf/2503.13217)] [[project](https://selen-suyue.github.io/DspNet/)] 
- [2025] Learning to Play Piano in the Real World [[paper](https://arxiv.org/pdf/2503.15481)] [[project](https://lasr.org/research/learning-to-play-piano)] 
- [2025] CCDP: Composition of Conditional Diffusion Policies with Guided Sampling [[paper](https://arxiv.org/pdf/2503.15386)] [[project](https://hri-eu.github.io/ccdp/)] 
- [2025] DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation [[paper](https://arxiv.org/pdf/2503.16806)] [[project](https://pku-epic.github.io/DyWA/)] 
- [2025] AdaWorld: Learning Adaptable World Models with Latent Actions [[paper](https://arxiv.org/pdf/2503.18938)] [[project](https://adaptable-world-model.github.io/)] 
- [2025] Visuo-Tactile Object Pose Estimation for a Multi-Finger Robot Hand with Low-Resolution In-Hand Tactile Sensing [[paper](https://arxiv.org/pdf/2503.19893)]
- [2025] Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels [[paper](https://arxiv.org/pdf/2503.22634)]
- [2025] ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning [[paper](https://arxiv.org/pdf/2503.21860)] [[project](https://maniptrans.github.io/)] 
- [2025] Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.24361)] [[project](https://co-training.github.io/)] 
- [2025] HACTS: a Human-As-Copilot Teleoperation System for Robot Learning [[paper](https://arxiv.org/pdf/2503.24070)] 
- [2025] ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos [[paper](https://arxiv.org/pdf/2503.23877)] [[project](https://zeromimic.github.io/)] 
- [2025] Learning Coordinated Bimanual Manipulation Policies using State Diffusion and Inverse Dynamics Models [[paper](https://arxiv.org/pdf/2503.23271)]
- [2025] Unified Video Action Model [[paper](https://arxiv.org/pdf/2503.00200)] [[project](https://unified-video-action-model.github.io/)] 
- [2025] Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets [[paper](https://arxiv.org/pdf/2504.02792)] [[project](https://weirdlabuw.github.io/uwm/)]
- [2025] RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics [[paper](https://arxiv.org/pdf/2504.02069)]
- [2025] Slot-Level Robotic Placement via Visual Imitation from Single Human Video [[paper](https://arxiv.org/pdf/2504.01959)] [[project](https://ddshan.github.io/slerp/)]
- [2025] Robust Dexterous Grasping of General Objects from Single-view Perception [[paper](https://arxiv.org/pdf/2504.05287)] [[project](https://zdchan.github.io/Robust_DexGrasp/)]
- [2025] Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation [[paper](https://arxiv.org/pdf/2504.06961)] [[project](https://tea-lab.github.io/TwoByTwo/)]
- [2025] ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping [[paper](https://arxiv.org/pdf/2504.10857)] [[project](https://sh8.io/#/zerograsp)]
- [2025] Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation [[paper](https://arxiv.org/pdf/2504.13175)] [[project](https://yangsizhe.github.io/robosplat/)]
- [2025] Grasping Deformable Objects via Reinforcement Learning with Cross-Modal Attention to Visuo-Tactile Inputs [[paper](https://arxiv.org/pdf/2504.15595)]
- [2025] Few-Shot Vision-Language Action-Incremental Policy Learning [[paper](https://arxiv.org/pdf/2504.15517)] [[project](https://github.com/codeshop715/FSAIL)]
- [2025] Latent Diffusion Planning for Imitation Learning [[paper](https://arxiv.org/pdf/2504.16925)] [[project](https://amberxie88.github.io/ldp/)]
- [2025] Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models [[paper](https://arxiv.org/pdf/2504.16843)]


### 2024

- [2024] Learning Robotic Manipulation Policies from Point Clouds with Conditional Flow Matching [[paper](https://arxiv.org/pdf/2409.07343)]
- [2024] Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning [[paper](https://arxiv.org/abs/2402.02500)]
- [2024] 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations [[paper](https://arxiv.org/pdf/2403.03954)]
- [2024] Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning [[paper](https://arxiv.org/pdf/2407.01531)]
- [2024] ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.01586)]
- [2024] 3d diffuser actor: Policy diffusion with 3d scene representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Diffusion Policy Policy Optimization [[paper](https://arxiv.org/pdf/2409.00588)]
- [2024] Language-Guided Object-Centric Diffusion Policy for Collision-Aware Robotic Manipulation [[paper](https://arxiv.org/pdf/2407.00451)]
- [2024] EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning [[paper](https://arxiv.org/pdf/2407.01479)]
- [2024] Equivariant Diffusion Policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models [[paper](https://arxiv.org/pdf/2409.07163)]
- [2024] Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies [[paper](https://arxiv.org/pdf/2410.10803)]
- [2024] Motion Before Action: Diffusing Object Motion as Manipulation Condition [[paper](https://arxiv.org/pdf/2411.09658)]
- [2024] One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation [[paper](https://arxiv.org/pdf/2410.21257)]
- [2024] Consistency policy: Accelerated visuomotor policies via consistency distillation [[paper](https://arxiv.org/pdf/2405.07503)]
- [2024] SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation [[paper](https://arxiv.org/pdf/2411.00965)]
- [2024] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins [[paper](https://arxiv.org/pdf/2409.02920)]
- [2024] Few-Shot Task Learning through Inverse Generative Modeling [[paper](https://arxiv.org/pdf/2411.04987)]
- [2024] G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation [[paper](https://arxiv.org/pdf/2411.18369)]
- [2024] Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation [[paper](https://arxiv.org/pdf/2410.08001)]
- [2024] Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies [[paper](https://arxiv.org/pdf/2405.19424)]
- [2024] Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies [[paper](https://arxiv.org/pdf/2406.11740)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation [[paper](https://arxiv.org/pdf/2409.14411)]
- [2024] Data Scaling Laws in Imitation Learning for Robotic Manipulation [[paper](https://arxiv.org/pdf/2410.18647)]
- [2024] Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.pdf)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Learning universal policies via text-guided video generation [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)]
- [2024] Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning [[paper](https://arxiv.org/pdf/2307.01849)]
- [2024] 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation [[paper](https://arxiv.org/pdf/2306.17817)]
- [2024] GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy [[paper](https://arxiv.org/pdf/2410.17488)]
- [2024] Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation [[paper](https://arxiv.org/pdf/2411.18623)]
- [2024] Prediction with Action: Visual Policy Learning via Joint Denoising Process [[paper](https://arxiv.org/pdf/2411.18179)]
- [2024] Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations [[paper](https://arxiv.org/pdf/2412.14803)]
- [2024] Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling [[paper](https://arxiv.org/pdf/2408.17355)]
- [2024] Streaming Diffusion Policy: Fast Policy Synthesis with Variable Noise Diffusion Models [[paper](https://arxiv.org/pdf/2406.04806)]
- [2024] CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction [[paper](https://arxiv.org/pdf/2412.06782)]
- [2024] In-Context Imitation Learning via Next-Token Prediction [[paper](https://arxiv.org/pdf/2408.15980)] [[project](https://icrt.dev/)]

### 2023

- [2023] Diffusion policy: Visuomotor policy learning via action diffusion [[paper](https://arxiv.org/pdf/2303.04137)]
- [2023] Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods [[paper](https://arxiv.org/pdf/2308.03620)]

## Other Multimodal Large Language Model (MLLM)-based/related Embodied Learning

### 2025
- [2025] [**NVIDIA**] Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning [[paper](https://arxiv.org/pdf/2503.15558)] [[project](https://github.com/nvidia-cosmos/cosmos-reason1)] 
- [2025] EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks [[paper](https://arxiv.org/pdf/2503.11089)]
- [2025] RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation [[paper](https://arxiv.org/pdf/2503.01616)] [[project](https://henryhcliu.github.io/robodexvlm/)] 
- [2025] Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation [[paper](https://arxiv.org/pdf/2502.16707v1)]
- [2025] RoboGrasp: A Universal Grasping Policy for Robust Robotic Control [[paper](https://arxiv.org/pdf/2502.03072)]
- [2025] VLP: Vision-Language Preference Learning for Embodied Manipulation [[paper](https://arxiv.org/pdf/2502.11918)]
- [2025] 3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds [[paper](https://arxiv.org/pdf/2502.20041)]
- [2025] CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments [[paper](https://arxiv.org/pdf/2503.00729)]
- [2025] TRACE: A Self-Improving Framework for Robot Behavior Forecasting with Vision-Language Models [[paper](https://arxiv.org/pdf/2503.00761)] [[project](https://trace-robotics.github.io/)]
- [2025] AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter [[paper](https://arxiv.org/pdf/2503.00778)] [[project](https://eqcy.github.io/affordgrasp/)]
- [2025] Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation [[paper](https://arxiv.org/pdf/2503.01700)] [[project](https://yongchao98.github.io/Code-Symbol-Planner/)]
- [2025] Large Language Models as Natural Selector for Embodied Soft Robot Design [[paper](https://arxiv.org/pdf/2503.02249)] [[project](https://github.com/AisenGinn/evogym_data_generation)]
- [2025] OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown Environments [[paper](https://arxiv.org/pdf/2503.02106)]
- [2025] Bridging VLM and KMP: Enabling Fine-grained robotic manipulation via Semantic Keypoints Representation [[paper](https://arxiv.org/pdf/2503.02748)]
- [2025] FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic Instruction Following [[paper](https://arxiv.org/pdf/2503.02698)] [[project](https://instruction-following-project.github.io/)]
- [2025] UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search and Rescue [[paper](https://arxiv.org/pdf/2503.02465)] [[video](https://www.youtube.com/watch?v=KJqQGKKt1xY&feature=youtu.be)]
- [2025] UAV-VLPA: A Vision-Language-Path-Action System for Optimal Route Generation on a Large Scales [[paper](https://arxiv.org/pdf/2503.02454)]
- [2025] Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation [[paper](https://arxiv.org/pdf/2503.03556)] [[project](https://zhuxmmm.github.io/Afford-X/)] 
- [2025] Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models [[paper](https://arxiv.org/pdf/2503.04280)] [[video](https://www.youtube.com/watch?v=kaTxZ4oqD6Y&feature=youtu.be)] 
- [2025] LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation [[paper](https://arxiv.org/pdf/2503.03890)]
- [2025] Learning Generalizable Language-Conditioned Cloth Manipulation from Long Demonstrations [[paper](https://arxiv.org/pdf/2503.04557)] [[project](https://sites.google.com/view/gen-cloth)] 
- [2025] Look Before You Leap: Using Serialized State Machine for Language Conditioned Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.05114)]
- [2025] Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.05064)] 
- [2025] AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning [[paper](https://arxiv.org/pdf/2503.07557)] [[project](https://github.com/Yanko96/AutoSpatial)] 
- [2025] AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance [[paper](https://arxiv.org/pdf/2503.07360)] [[project](https://isee-laboratory.github.io/AffordDexGrasp/)]
- [2025] Self-Corrective Task Planning by Inverse Prompting with Large Language Models [[paper](https://arxiv.org/pdf/2503.07317)]
- [2025] HELM: Human-Preferred Exploration with Language Models [[paper](https://arxiv.org/pdf/2503.07006)]
- [2025] SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning [[paper](https://arxiv.org/pdf/2503.06892)]
- [2025] Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception [[paper](https://arxiv.org/pdf/2503.06866)]
- [2025] RoboDesign1M: A Large-scale Dataset for Robot Design Understanding [[paper](https://arxiv.org/pdf/2503.06796)]
- [2025] STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems [[paper](https://arxiv.org/pdf/2503.06060)]
- [2025] MatchMaker: Automated Asset Generation for Robotic Assembly [[paper](https://arxiv.org/pdf/2503.05887)]
- [2025] Object-Centric World Model for Language-Guided Manipulation [[paper](https://arxiv.org/pdf/2503.06170)]
- [2025] KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation [[paper](https://arxiv.org/pdf/2503.10546)] [[project](https://kuda-dynamics.github.io/)]
- [2025] IMPACT : Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models [[paper](https://arxiv.org/pdf/2503.10110)] [[project](https://impact-planning.github.io/)]
- [2025] Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation [[paper](https://arxiv.org/pdf/2503.09758)] [[project](https://sites.google.com/view/SAMALM)]
- [2025] PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability [[paper](https://arxiv.org/pdf/2503.08481)] 
- [2025] MetaFold: Language-Guided Multi-Category Garment Folding Framework via Trajectory Generation and Foundation Model [[paper](https://arxiv.org/pdf/2503.08372)] 
- [2025] NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model [[paper](https://arxiv.org/pdf/2503.09335)] [[project](https://github.com/laiyuzhi/NVP-HRI.git)]
- [2025] MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System for Implicit Intention Recognition and Task Execution [[paper](https://arxiv.org/pdf/2503.13250)]
- [2025] HybridGen: VLM-Guided Hybrid Planning for Scalable Data Generation of Imitation Learning [[paper](https://arxiv.org/pdf/2503.13171)]
- [2025] Free-form language-based robotic reasoning and grasping [[paper](https://arxiv.org/pdf/2503.13082)] [[project](https://tev-fbk.github.io/FreeGrasp/)]
- [2025] Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility via Affordance-Guided, Self-Consistent MLLMs for Food Preparation Task Planning [[paper](https://arxiv.org/pdf/2503.13055)]
- [2025] VISO-Grasp: Vision-Language Informed Spatial Object-centric 6-DoF Active View Planning and Grasping in Clutter and Invisibility [[paper](https://arxiv.org/pdf/2503.12609)]
- [2025] Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation [[paper](https://arxiv.org/pdf/2503.11999)]
- [2025] GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions [[paper](https://arxiv.org/pdf/2503.16013)]
- [2025] Safety Aware Task Planning via Large Language Models in Robotics [[paper](https://arxiv.org/pdf/2503.15707)]
- [2025] LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language [[paper](https://arxiv.org/pdf/2503.17309)] [[project](https://github.com/Kchu/LLM-MAP)] 
- [2025] Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning [[paper](https://arxiv.org/pdf/2503.17125)] [[project](https://lamour-rl.github.io/)] 
- [2025] RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot Segmentation and Background Generation [[paper](https://arxiv.org/pdf/2503.18738)] [[project](https://roboengine.github.io/)] 
- [2025] IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes [[paper](https://arxiv.org/pdf/2503.17406)]
- [2025] Cooking Task Planning using LLM and Verified by Graph Network [[paper](https://arxiv.org/pdf/2503.21564)]
- [2025] Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights [[paper](https://arxiv.org/pdf/2504.00839)]
- [2025] Visual Environment-Interactive Planning for Embodied Complex-Question Answering [[paper](https://arxiv.org/pdf/2504.00775)]
- [2025] GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill [[paper](https://arxiv.org/pdf/2504.04191)] [[project](https://jiemingcui.github.io/grove/)] 
- [2025] Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks [[paper](https://arxiv.org/pdf/2503.21696)] [[project](https://embodied-reasoner.github.io/)] 
- [2025] Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning [[paper](https://arxiv.org/pdf/2504.00907)]
- [2025] EmbodiedAgent: A Scalable Hierarchical Approach to Overcome Practical Challenge in Multi-Robot Control [[paper](https://arxiv.org/pdf/2504.10030v1)]
- [2025] Trajectory Adaptation Using Large Language Models [[paper](https://arxiv.org/pdf/2504.12755)]
- [2025] Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models [[paper](https://arxiv.org/pdf/2504.13351)] [[project](https://chain-of-modality.github.io/)] 
- [2025] SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement [[paper](https://arxiv.org/pdf/2504.20459)] [[project](https://sites.google.com/asu.edu/sas-llm/)] 


### 2024

- [2024] Building Cooperative Embodied Agents Modularly with Large Language Models [[paper](https://arxiv.org/pdf/2307.02485)] [[project](https://vis-www.cs.umass.edu/Co-LLM-Agents/)]
- [2024] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.11548)]
- [2024] OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [[paper](https://arxiv.org/pdf/2501.03841)]
- [2024] Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice [[paper](https://arxiv.org/pdf/2412.10694)]
- [2024] Towards Open-World Grasping with Large Vision-Language Models [[paper](https://arxiv.org/pdf/2406.18722v4)]
- [2024] ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter [[paper](https://arxiv.org/pdf/2407.11298v1)]
- [2024] MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World [[paper](https://arxiv.org/pdf/2401.08577)]
- [2024] Physically Grounded Vision-Language Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2309.02561)]
- [2024] Eureka: Human-Level Reward Design via Coding Large Language Models [[paper](https://arxiv.org/pdf/2310.12931)]
- [2024] Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration [[paper](https://arxiv.org/pdf/2405.14314)]
- [2024] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [[paper](https://say-can.github.io/assets/palm_saycan.pdf)]


### 2023
- [2023] LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models [[paper](https://arxiv.org/pdf/2212.04088)]
- [2023] LLM+P: Empowering Large Language Models with Optimal Planning Proficiency [[paper](https://arxiv.org/pdf/2304.11477)]
- [2023] Code as Policies: Language Model Programs for Embodied Control [[paper](https://arxiv.org/pdf/2209.07753)]

### 2022
- [2022] Inner Monologue: Embodied Reasoning through Planning with Language Models [[paper](https://arxiv.org/pdf/2207.05608)]


## Physics-aware Policy
- [2025] Surface-Based Manipulation [[paper](https://arxiv.org/pdf/2502.19389)]

## Sim-to-Real Transfer

- [2025] RE3SIM: Generating High-Fidelity Simulation Data via 3D-Photorealistic Real-to-Sim for Robotic Manipulation [[paper](https://arxiv.org/pdf/2502.08645)]
- [2025] VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion [[paper](https://arxiv.org/pdf/2502.01536)]
- [2025] A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards [[paper](https://arxiv.org/pdf/2502.08643)]
- [2025] A Distributional Treatment of Real2Sim2Real for Vision-Driven Deformable Linear Object Manipulation [[paper](https://arxiv.org/pdf/2502.18615)]
- [2025] Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids [[paper](https://arxiv.org/pdf/2502.20396)] [[project](https://toruowo.github.io/recipe/)]
- [2025] Impact of Static Friction on Sim2Real in Robotic Reinforcement Learning [[paper](https://arxiv.org/pdf/2503.01255)]
- [2025] Few-shot Sim2Real Based on High Fidelity Rendering with Force Feedback Teleop [[paper](https://arxiv.org/pdf/2503.01301)]
- [2025] An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy Transfer with Differentiable Simulation [[paper](https://arxiv.org/pdf/2503.10118)] [[project](https://github.com/sunnyshi0310/RSR-MJX)]

## Benchmark
- [2025] LocoMuJoCo [[documentation](https://loco-mujoco.readthedocs.io/en/latest/)] [[project](https://github.com/robfiras/loco-mujoco)]
- [2025] RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning [[paper](https://roboverseorg.github.io/static/pdfs/paper_supp_20250405_1820.pdf)] [[project](https://github.com/RoboVerseOrg/RoboVerse)]
- [2025] AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World [[paper](https://arxiv.org/pdf/2503.24278)] [[project](https://auto-eval.github.io/)]
- [2025] RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints [[paper](https://arxiv.org/pdf/2503.16408)] [[project](https://iranqin.github.io/robofactory/)]
- [2025] OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation [[paper](https://arxiv.org/pdf/2502.18041)]
- [2025] BOSS: Benchmark for Observation Space Shift in Long-Horizon Task [[paper](http://arxiv.org/pdf/2502.15679)]
- [2025] OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics [[paper](https://arxiv.org/pdf/2502.09238)]
- [2024] EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models [[paper](https://arxiv.org/abs/2406.05756)]
- [2024] VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Task [[paper](https://arxiv.org/pdf/2412.18194)]
- [2024] Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations [[paper](https://arxiv.org/pdf/2402.14606)]

## Simulator
- [2025] MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation [[paper](https://arxiv.org/pdf/2503.02834)] [[project](https://github.com/michaal94/MuBlE)]
- [2025] MuJoCo Playground [[paper](https://arxiv.org/pdf/2502.08844)]
- [2024] Genesis: A Generative and Universal Physics Engine for Robotics and Beyond [[paper](https://genesis-embodied-ai.github.io/)]
- [2024] Maniskill V1-V3 [[v1](https://arxiv.org/pdf/2107.14483)] [[v2](https://arxiv.org/pdf/2302.04659)] [[v3](https://arxiv.org/pdf/2410.00425)]
- [2024] Nvidia Isaac [[Isaac Lab](https://github.com/isaac-sim/IsaacLab)] [[Isaac Sim](https://docs.isaacsim.omniverse.nvidia.com/latest/index.html)] [[Isaac Gym](https://developer.nvidia.com/isaac-gym)]
- [2022] Mojoco [[document](https://mujoco.readthedocs.io/en/stable/overview.html)]


## Ralated Works
- Awesome VLA for Robotics [[repo](https://github.com/Jiaaqiliu/Awesome-VLA-Robotics)]
- Awesome-Generalist-Agents [[repo](https://github.com/cheryyunl/awesome-generalist-agents)]
- Awesome-LLM-Robotics [[repo](https://github.com/GT-RIPL/Awesome-LLM-Robotics)]
- Awesome World Models for Robotics [[repo](https://github.com/leofan90/Awesome-World-Models)]


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jonyzhang2023/awesome-embodied-vla-va-vln&type=Date)](https://star-history.com/#jonyzhang2023/awesome-embodied-vla-va-vln&Date)
