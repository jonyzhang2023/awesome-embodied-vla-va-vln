# awesome-embodied-vla/va [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Vision Language Action (VLA) Models


### 2025 

- [2025] EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation [[paper](https://arxiv.org/pdf/2501.01895)]
  


### 2024

- [2024] Ï€0: A Vision-Language-Action Flow Model for General Robot Control [[paper](https://www.physicalintelligence.company/download/pi0.pdf)]
- [2024] RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation [[paper](https://arxiv.org/pdf/2410.07864)]
- [2024] OpenVLA: An Open-Source Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2406.09246)]
- [2024] Octo: An Open-Source Generalist Robot Policy [[paper](https://arxiv.org/pdf/2405.12213)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] RT-H: Action Hierarchies Using Language [[paper](https://arxiv.org/pdf/2403.01823v1)]
- [2024] Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models [[paper](https://arxiv.org/abs/2412.14058)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] Baku: An Efficient Transformer for Multi-Task Policy Learning [[paper](https://arxiv.org/pdf/2406.07539v1)]
- [2024] Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals [[paper](https://arxiv.org/pdf/2407.05996)]
- [2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.12514)]
- [2024] Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression [[paper](https://arxiv.org/pdf/2412.03293)]
- [2024] CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation [[paper](https://www.arxiv.org/pdf/2411.19650)]
- [2024] 3D-VLA: A 3D Vision-Language-Action Generative World Model [[paper](https://arxiv.org/pdf/2403.09631)]
- [2024] Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations [[paper](https://arxiv.org/pdf/2405.06039)]
- [2024] An Embodied Generalist Agent in 3D World [[paper](https://arxiv.org/pdf/2311.12871)]
- [2024] RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2412.07215)]


### 2023 
- [2023] RT-1: Robotics Transformer for Real-World Control at Scale [[paper](https://arxiv.org/pdf/2212.06817)]
- [2023] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper](https://arxiv.org/pdf/2307.15818)]

## Vision Action (VA) Models

### 2024

- [2024] 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations [[paper](https://arxiv.org/pdf/2403.03954)]
- [2024] Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning [[paper](https://arxiv.org/pdf/2407.01531)]
- [2024] ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.01586)]
- [2024] 3d diffuser actor: Policy diffusion with 3d scene representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Diffusion Policy Policy Optimization [[paper](https://arxiv.org/pdf/2409.00588)]
- [2024] Language-Guided Object-Centric Diffusion Policy for Collision-Aware Robotic Manipulation [[paper](https://arxiv.org/pdf/2407.00451)]
- [2024] EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning [[paper](https://arxiv.org/pdf/2407.01479)]
- [2024] Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models [[paper](https://arxiv.org/pdf/2409.07163)]
- [2024] Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies [[paper](https://arxiv.org/pdf/2410.10803)]
- [2024] Motion Before Action: Diffusing Object Motion as Manipulation Condition [[paper](https://arxiv.org/pdf/2411.09658)]
- [2024] One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation [[paper](https://arxiv.org/pdf/2410.21257)]
- [2024] Consistency policy: Accelerated visuomotor policies via consistency distillation [[paper](https://arxiv.org/pdf/2405.07503)]
- [2024] SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation [[paper](https://arxiv.org/pdf/2411.00965)]
- [2024] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins [[paper](https://arxiv.org/pdf/2409.02920)]
- [2024] Few-Shot Task Learning through Inverse Generative Modeling [[paper](https://arxiv.org/pdf/2411.04987?)]
- [2024] G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation [[paper](https://arxiv.org/pdf/2411.18369)]
- [2024] Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation [[paper](https://arxiv.org/pdf/2410.08001)]
- [2024] Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies [[paper](https://arxiv.org/pdf/2405.19424)]
- [2024] Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies [[paper](https://arxiv.org/pdf/2406.11740)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation [[paper](https://arxiv.org/pdf/2409.14411)]
- [2024] Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.pdf)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Learning universal policies via text-guided video generation [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)]




### 2023

- [2023] Diffusion policy: Visuomotor policy learning via action diffusion [[paper](https://arxiv.org/pdf/2303.04137)]